import sqlite3
import json
import os
import argparse
from typing import Dict, Any, List, Tuple

def convert_tools_to_string(obj):
    """é€’å½’å°† 'tools' å­—æ®µï¼ˆè‹¥ä¸ºåˆ—è¡¨ï¼‰è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ã€‚"""
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k == "tools" and isinstance(v, list):
                obj[k] = json.dumps(v, ensure_ascii=False)
            else:
                convert_tools_to_string(v)
    elif isinstance(obj, list):
        for it in obj:
            convert_tools_to_string(it)
    return obj

def validate_sharegpt(data: Dict[str, Any]) -> None:
    """éªŒè¯ ShareGPT æ‰©å±•ç»“æ„ï¼ˆåŒ…å« function_call/observationï¼‰ã€‚æœ‰é—®é¢˜ç›´æ¥æŠ›å¼‚å¸¸ã€‚"""
    if not isinstance(data, dict):
        raise ValueError("æ ¹å¯¹è±¡ä¸æ˜¯å­—å…¸")
    if "conversations" not in data or not isinstance(data["conversations"], list):
        raise ValueError("ç¼ºå°‘æˆ–é”™è¯¯çš„ conversations")
    for conv in data["conversations"]:
        if not isinstance(conv, dict):
            raise ValueError("conversation æ¡ç›®ä¸æ˜¯å¯¹è±¡")
        if "from" not in conv or "value" not in conv:
            raise ValueError("conversation æ¡ç›®ç¼ºå°‘ from/value")
    if "system" not in data or not isinstance(data["system"], str):
        raise ValueError("ç¼ºå°‘æˆ–é”™è¯¯çš„ system å­—æ®µ")
    if "tools" not in data:
        raise ValueError("ç¼ºå°‘ tools å­—æ®µ")
    # tools å¯ä¸ºå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
    if isinstance(data["tools"], list):
        data["tools"] = json.dumps(data["tools"], ensure_ascii=False)
    elif not isinstance(data["tools"], str):
        raise ValueError("tools å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")
    # æ ¡éªŒ tools å­—ç¬¦ä¸²æ˜¯åˆæ³• JSON
    try:
        json.loads(data["tools"])
    except Exception:
        raise ValueError("tools å­—ç¬¦ä¸²ä¸æ˜¯æœ‰æ•ˆ JSON")

def is_function_call_only(conv_list: List[Dict[str, Any]]) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºä»…å·¥å…·è°ƒç”¨æ ·æœ¬ï¼šå­˜åœ¨ function_callï¼Œä¸”ä¸å­˜åœ¨ gpt/assistant æ–‡æœ¬ã€‚"""
    if not isinstance(conv_list, list):
        return False
    has_fc = any(isinstance(m, dict) and m.get("from") == "function_call" for m in conv_list)
    has_gpt = any(isinstance(m, dict) and m.get("from") in ("gpt", "assistant") for m in conv_list)
    return bool(has_fc and not has_gpt)

def has_tool_use(conv_list: List[Dict[str, Any]]) -> bool:
    return any(isinstance(m, dict) and m.get("from") in ("function_call", "observation") for m in conv_list)

def truncate_observation_inplace(data: Dict[str, Any], max_len: int) -> None:
    """å¯¹ observation çš„ value è¿›è¡Œé•¿åº¦æˆªæ–­ï¼ˆä»…å¯¼å‡ºæ—¶ç”Ÿæ•ˆï¼Œä¸æ”¹æ•°æ®åº“ï¼‰ã€‚"""
    if max_len <= 0:
        return
    convs = data.get("conversations", [])
    if not isinstance(convs, list):
        return
    for m in convs:
        if isinstance(m, dict) and m.get("from") == "observation":
            v = m.get("value")
            if isinstance(v, str) and len(v) > max_len:
                m["value"] = v[:max_len] + "\n... [truncated at export] ..."

def fetch_rows(conn: sqlite3.Connection, table: str, model_filter: str = "") -> List[Tuple[str]]:
    """ä»æŒ‡å®šè¡¨è·å– conversation åˆ—å†…å®¹ã€‚"""
    cur = conn.cursor()
    if table == "confirmed_interactions":
        base_sql = "SELECT model, conversation FROM confirmed_interactions ORDER BY confirmed_timestamp"
    else:
        base_sql = "SELECT model, conversation FROM interactions ORDER BY timestamp"
    if model_filter:
        cur.execute(base_sql)
        all_rows = cur.fetchall()
        return [(m, c) for (m, c) in all_rows if model_filter in (m or "")]
    cur.execute(base_sql)
    return cur.fetchall()

def process_conversations(
    db_path: str = "interactions.db",
    table: str = "interactions",
    output_file: str = "conversations.jsonl",
    invalid_file: str = "invalid_conversations.jsonl",
    only_function_call_only: bool = False,
    truncate_observation: int = 0,
    model_filter: str = "",
    max_records: int = 0
) -> None:
    """å¯¼å‡º + æ ¡éªŒï¼šä»æ•°æ®åº“å¯¼å‡º ShareGPT æ‰©å±•æ ¼å¼ JSONLï¼Œå¹¶è¾“å‡ºç»Ÿè®¡ã€‚"""
    conn = None
    if table not in ("interactions", "confirmed_interactions"):
        raise ValueError("table å¿…é¡»ä¸º interactions æˆ– confirmed_interactions")

    try:
        conn = sqlite3.connect(db_path)
        rows = fetch_rows(conn, table, model_filter)

        if not rows:
            print("âŒ æ²¡æœ‰åŒ¹é…çš„æ•°æ®")
            return

        valid_count = 0
        invalid_count = 0
        fc_only_count = 0
        with_tool_count = 0
        with_gpt_count = 0
        errors: List[str] = []
        total = 0

        with open(output_file, "w", encoding="utf-8") as f_ok, open(invalid_file, "w", encoding="utf-8") as f_bad:
            for idx, (model, conv_str) in enumerate(rows, 1):
                if max_records and valid_count + invalid_count >= max_records:
                    break
                total += 1
                try:
                    data = json.loads(conv_str)

                    # ç»Ÿä¸€ tools å­—æ®µä¸ºå­—ç¬¦ä¸²
                    convert_tools_to_string(data)

                    # æ ¡éªŒç»“æ„
                    validate_sharegpt(data)

                    # æŒ‡æ ‡ç»Ÿè®¡
                    conv_list = data.get("conversations", [])
                    if has_tool_use(conv_list):
                        with_tool_count += 1
                    if any(isinstance(m, dict) and m.get("from") in ("gpt", "assistant") for m in conv_list):
                        with_gpt_count += 1

                    fc_only = is_function_call_only(conv_list)
                    if only_function_call_only and not fc_only:
                        # è¿‡æ»¤æ‰é function_call-only
                        continue
                    if fc_only:
                        fc_only_count += 1

                    # å¯¼å‡ºå‰å¯¹ observation æˆªæ–­ï¼ˆä¸å½±å“åŸæ•°æ®ï¼‰
                    if truncate_observation > 0:
                        truncate_observation_inplace(data, truncate_observation)

                    # å†™å…¥æœ‰æ•ˆæ ·æœ¬
                    f_ok.write(json.dumps(data, ensure_ascii=False) + "\n")
                    valid_count += 1
                except Exception as e:
                    invalid_count += 1
                    # åŸæ ·å†™å‡ºæ— æ•ˆè®°å½•ï¼Œä¾¿äºåç»­æ’æŸ¥
                    f_bad.write(conv_str + "\n")
                    if len(errors) < 20:
                        errors.append(f"è®°å½• {idx}: {repr(e)}")

        print("âœ… å¯¼å‡ºå®Œæˆ")
        print(f"ğŸ“¦ æ¥æºè¡¨: {table}")
        print(f"ğŸ§® æ€»è¯»å–æ¡æ•°: {total}")
        print(f"ğŸŸ¢ æœ‰æ•ˆ: {valid_count}  | ğŸ”´ æ— æ•ˆ: {invalid_count}")
        if valid_count:
            print(f"ğŸ› ï¸ å«å·¥å…·è°ƒç”¨: {with_tool_count} ({with_tool_count / max(valid_count,1):.1%} of valid)")
            print(f"ğŸ—£ï¸ å«åŠ©æ‰‹æ–‡æœ¬: {with_gpt_count} ({with_gpt_count / max(valid_count,1):.1%} of valid)")
            print(f"ğŸ§© function_call-only: {fc_only_count} ({fc_only_count / max(valid_count,1):.1%} of valid)")
        if errors:
            print("\néƒ¨åˆ†é”™è¯¯æ ·ä¾‹ï¼ˆæœ€å¤š20æ¡ï¼‰ï¼š")
            for e in errors:
                print("-", e)
        if os.path.exists(output_file):
            print(f"\nğŸ“ å¯¼å‡ºæ–‡ä»¶: {output_file}  å¤§å°: {os.path.getsize(output_file)/1024:.2f} KB")
        if os.path.exists(invalid_file):
            print(f"ğŸ§¾ æ— æ•ˆæ ·æœ¬: {invalid_file}  å¤§å°: {os.path.getsize(invalid_file)/1024:.2f} KB")

    finally:
        try:
            conn and conn.close()
        except Exception:
            pass

def parse_args():
    p = argparse.ArgumentParser(description="å¯¼å‡º/æ ¡éªŒ ShareGPT æ‰©å±•æ•°æ®ï¼ˆæ”¯æŒ function_call/observationï¼‰")
    p.add_argument("--db", default="interactions.db", help="æ•°æ®åº“è·¯å¾„")
    p.add_argument("--table", default="interactions", choices=["interactions", "confirmed_interactions"], help="å¯¼å‡ºæ¥æºè¡¨")
    p.add_argument("--output", default="conversations.jsonl", help="æœ‰æ•ˆæ ·æœ¬å¯¼å‡ºæ–‡ä»¶")
    p.add_argument("--invalid", default="invalid_conversations.jsonl", help="æ— æ•ˆæ ·æœ¬å¯¼å‡ºæ–‡ä»¶")
    p.add_argument("--only-function-call-only", action="store_true", help="ä»…å¯¼å‡º function_call-only æ ·æœ¬")
    p.add_argument("--truncate-observation", type=int, default=0, help="å¯¹ observation çš„ value è¿›è¡Œé•¿åº¦æˆªæ–­ï¼ˆ0 è¡¨ç¤ºä¸æˆªæ–­ï¼‰")
    p.add_argument("--model-filter", default="", help="æŒ‰æ¨¡å‹ååŒ…å«åŒ¹é…è¿‡æ»¤ï¼ˆç®€å•åŒ…å«åŒ¹é…ï¼‰")
    p.add_argument("--max-records", type=int, default=0, help="æœ€å¤šå¤„ç†å¤šå°‘æ¡ï¼ˆ0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    if not os.path.exists(args.db):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶: {args.db}")
        raise SystemExit(1)
    print("ğŸš€ å¼€å§‹å¯¼å‡º/æ ¡éªŒ ...")
    process_conversations(
        db_path=args.db,
        table=args.table,
        output_file=args.output,
        invalid_file=args.invalid,
        only_function_call_only=args.only_function_call_only,
        truncate_observation=args.truncate_observation,
        model_filter=args.model_filter,
        max_records=args.max_records
    )
    print("ğŸ‰ å®Œæˆ")