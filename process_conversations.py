import sqlite3
import json
import os
import argparse
import uuid
from typing import Dict, Any, List, Tuple, cast

def convert_tools_to_string(obj):
    """é€’å½’å°† 'tools' å­—æ®µï¼ˆè‹¥ä¸ºåˆ—è¡¨ï¼‰è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ã€‚"""
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k == "tools" and isinstance(v, list):
                obj[k] = json.dumps(v, ensure_ascii=False)
            else:
                convert_tools_to_string(v)
    elif isinstance(obj, list):
        for it in obj:
            convert_tools_to_string(it)
    return obj

def validate_sharegpt(data: Dict[str, Any]) -> None:
    """éªŒè¯ ShareGPT æ‰©å±•ç»“æ„ï¼ˆåŒ…å« function_call/observationï¼‰ã€‚æœ‰é—®é¢˜ç›´æ¥æŠ›å¼‚å¸¸ã€‚"""
    if not isinstance(data, dict):
        raise ValueError("æ ¹å¯¹è±¡ä¸æ˜¯å­—å…¸")
    if "conversations" not in data or not isinstance(data["conversations"], list):
        raise ValueError("ç¼ºå°‘æˆ–é”™è¯¯çš„ conversations")
    for conv in data["conversations"]:
        if not isinstance(conv, dict):
            raise ValueError("conversation æ¡ç›®ä¸æ˜¯å¯¹è±¡")
        if "from" not in conv or "value" not in conv:
            raise ValueError("conversation æ¡ç›®ç¼ºå°‘ from/value")
    if "system" not in data or not isinstance(data["system"], str):
        raise ValueError("ç¼ºå°‘æˆ–é”™è¯¯çš„ system å­—æ®µ")
    if "tools" not in data:
        raise ValueError("ç¼ºå°‘ tools å­—æ®µ")
    # tools å¯ä¸ºå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
    if isinstance(data["tools"], list):
        data["tools"] = json.dumps(data["tools"], ensure_ascii=False)
    elif not isinstance(data["tools"], str):
        raise ValueError("tools å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")
    # æ ¡éªŒ tools å­—ç¬¦ä¸²æ˜¯åˆæ³• JSON
    try:
        json.loads(data["tools"])
    except Exception:
        raise ValueError("tools å­—ç¬¦ä¸²ä¸æ˜¯æœ‰æ•ˆ JSON")

def sharegpt_to_openai_messages(conv_list: List[Dict[str, Any]], system_text: str = "") -> List[Dict[str, Any]]:
    """
    å°† ShareGPT æ‰©å±• conversations è½¬ä¸º OpenAI Chat messagesã€‚
    è§„åˆ™ï¼š
    - human -> user
    - gpt/assistant -> assistantï¼ˆcontent ä¸ºæ–‡æœ¬ï¼‰
    - function_call -> æ”¶é›†åˆ° pending tool_callsï¼Œç¨åä½œä¸ºä¸€æ¡ assistantï¼ˆcontent=""ï¼‰çš„ tool_calls ä¸€å¹¶è¾“å‡º
    - observation -> role=toolï¼Œå°½é‡ä¸æœ€è¿‘æœªé…å¯¹çš„ tool_call æŒ‰é¡ºåºé…å¯¹ï¼›è‹¥ç¼º idï¼Œåˆ™è¡¥ä¸€ä¸ª
    - system_text éç©ºæ—¶ä½œä¸ºé¦–æ¡ role=system
    """
    messages: List[Dict[str, Any]] = []
    if isinstance(system_text, str) and system_text.strip():
        messages.append({"role": "system", "content": system_text})

    def normalize_tool_call(value: Any, idx: int) -> Dict[str, Any]:
        # value å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ–å¯¹è±¡ï¼ŒåŒ…å« {id?, type, function:{name, arguments}}
        tc = value
        if isinstance(value, str):
            try:
                tc = json.loads(value)
            except Exception:
                # å…œåº•ï¼šä½œä¸ºæœªçŸ¥å‡½æ•°åã€åŸä¸²ä½œä¸ºå‚æ•°
                return {
                    "id": f"toolcall_{idx}",
                    "type": "function",
                    "function": {"name": "unknown_tool", "arguments": json.dumps({"raw": value}, ensure_ascii=False)}
                }
        if not isinstance(tc, dict):
            tc = {}
        func = tc.get("function", {}) if isinstance(tc.get("function", {}), dict) else {}
        name = func.get("name") or "unknown_tool"
        args = func.get("arguments", {})
        if not isinstance(args, str):
            try:
                args = json.dumps(args, ensure_ascii=False)
            except Exception:
                args = json.dumps({"raw": str(func.get("arguments"))}, ensure_ascii=False)
        tool_id = tc.get("id") or f"toolcall_{idx}"
        return {"id": tool_id, "type": "function", "function": {"name": name, "arguments": args}}

    pending_tool_calls: List[Dict[str, Any]] = []
    orphan_counter = 0
    for i, item in enumerate(conv_list or []):
        if not isinstance(item, dict):
            continue
        frm = item.get("from")
        val = item.get("value", "")
        if frm == "human":
            # flush pending tool_calls before new user turn
            if pending_tool_calls:
                messages.append({"role": "assistant", "content": "", "tool_calls": pending_tool_calls})
                pending_tool_calls = []
            messages.append({"role": "user", "content": val if isinstance(val, str) else str(val)})
        elif frm in ("gpt", "assistant"):
            # flush pending tool_calls first
            if pending_tool_calls:
                messages.append({"role": "assistant", "content": "", "tool_calls": pending_tool_calls})
                pending_tool_calls = []
            if isinstance(val, str) and val.strip():
                messages.append({"role": "assistant", "content": val})
        elif frm == "function_call":
            tc = normalize_tool_call(val, len(pending_tool_calls) + 1)
            pending_tool_calls.append(tc)
        elif frm == "observation":
            tool_content = val if isinstance(val, str) else str(val)
            if pending_tool_calls:
                # pair with the earliest unpaired
                tc = pending_tool_calls.pop(0)
                # ensure assistant tool_calls emitted before tool response
                messages.append({"role": "assistant", "content": "", "tool_calls": [tc]})
                messages.append({"role": "tool", "tool_call_id": tc["id"], "content": tool_content})
            else:
                # orphan observation: synthesize an id for compatibility
                orphan_counter += 1
                oc_id = f"orphan_tool_{orphan_counter}"
                # ä¹Ÿå¯ç›´æ¥è¾“å‡ºæ—  tool_call_id çš„ tool æ¶ˆæ¯ï¼›æ­¤å¤„é‡‡ç”¨åˆæˆ idï¼Œå…¼å®¹æ€§æ›´å¥½
                messages.append({"role": "assistant", "content": "", "tool_calls": [{"id": oc_id, "type": "function", "function": {"name": "unknown_tool", "arguments": "{}"}}]})
                messages.append({"role": "tool", "tool_call_id": oc_id, "content": tool_content})
        else:
            # å…¶ä»– from å€¼å¿½ç•¥æˆ–ä½œä¸ºæ³¨é‡Šæ€§æ–‡æœ¬
            pass

    # flush trailing pending tool_calls
    if pending_tool_calls:
        messages.append({"role": "assistant", "content": "", "tool_calls": pending_tool_calls})

    return messages

def _safe_json_loads(s: str):
    try:
        return json.loads(s)
    except Exception:
        return None

def _guess_json_type(v):
    if isinstance(v, bool):
        return "boolean"
    if isinstance(v, int):
        return "integer"
    if isinstance(v, float):
        return "number"
    if isinstance(v, dict):
        return "object"
    if isinstance(v, list):
        return "array"
    return "string"

def derive_tools_schema(conv_list: List[Dict[str, Any]], tools_schema_mode: str, tools_raw: Any) -> List[Dict[str, Any]]:
    """
    åŸºäºç­–ç•¥ç”Ÿæˆ OpenAI tools åˆ—è¡¨ï¼š
    - auto: è‹¥ tools_raw å­˜åœ¨ä¸”ä¸ºåˆæ³• JSON åˆ—è¡¨åˆ™é€ä¼ ï¼›å¦åˆ™å½“ conv_list å« function_call æ—¶æŒ‰å®é™… arguments æ¨æ–­æœ€å° schema
    - yes:  ä¸ auto ç±»ä¼¼ï¼Œä½†å³ä¾¿æ—  function_call ä¹Ÿå°è¯•é€ä¼ /æ¨æ–­
    - no:   ä¸è¾“å‡º
    - derive: å¿½ç•¥ tools_rawï¼Œå®Œå…¨æŒ‰ conv_list æ¨æ–­
    """
    mode = (tools_schema_mode or "auto").lower()
    baked_tools = []
    if isinstance(tools_raw, str) and tools_raw.strip():
        parsed = _safe_json_loads(tools_raw)
        if isinstance(parsed, list):
            baked_tools = parsed

    if mode == "no":
        return []
    if mode == "auto":
        if baked_tools:
            return baked_tools
        has_fc = any(isinstance(it, dict) and it.get("from") == "function_call" for it in (conv_list or []))
        if not has_fc:
            return []
    elif mode == "yes":
        if baked_tools:
            return baked_tools
        # éœ€è¦æ¨æ–­
        pass
    elif mode == "derive":
        pass

    schema_map: Dict[str, Dict[str, Any]] = {}
    for it in (conv_list or []):
        if not (isinstance(it, dict) and it.get("from") == "function_call"):
            continue
        val = it.get("value")
        if isinstance(val, str):
            obj = _safe_json_loads(val)
        elif isinstance(val, dict):
            obj = val
        else:
            obj = None
        if not isinstance(obj, dict):
            obj = {}
        func = obj.get("function", {}) if isinstance(obj.get("function"), dict) else {}
        name = func.get("name") or "unknown_tool"
        args_raw = func.get("arguments", {})
        if isinstance(args_raw, str):
            args_obj = _safe_json_loads(args_raw)
            if not isinstance(args_obj, dict):
                args_obj = {}
        elif isinstance(args_raw, dict):
            args_obj = args_raw
        else:
            args_obj = {}

        if name not in schema_map:
            schema_map[name] = {
                "type": "function",
                "function": {
                    "name": name,
                    "parameters": {"type": "object", "properties": {}, "required": []}
                }
            }
        params = schema_map[name]["function"]["parameters"]
        props = params["properties"]
        req = params["required"]
        for k, v in args_obj.items():
            if k not in props:
                props[k] = {"type": _guess_json_type(v)}
            if k not in req:
                req.append(k)

    return list(schema_map.values())

def is_function_call_only(conv_list: List[Dict[str, Any]]) -> bool:
    """
    æŒ‰â€œæœ€åä¸€è½®â€åˆ¤æ–­ fc-onlyï¼š
    - åœ¨ conversations é¡ºåºä¸­ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ª from å±äº {human/gpt/function_call/observation} çš„æ¡ç›®
    - è‹¥æœ€åä¸€ä¸ªä¸º function_callï¼ˆå…¶åæ²¡æœ‰ gpt æ–‡æœ¬ï¼‰ï¼Œåˆ™è§†ä¸º fc-only
    æ³¨æ„ï¼šå†å²ä¸­å‡ºç°è¿‡ gpt æ–‡æœ¬ä¸å½±å“è¯¥è½®æ˜¯å¦ä¸º fc-only
    """
    if not isinstance(conv_list, list) or not conv_list:
        return False
    last_role = None
    for item in conv_list:
        if not isinstance(item, dict):
            continue
        role = item.get("from")
        if role in ("human", "gpt", "function_call", "observation", "assistant"):
            last_role = role
    return last_role == "function_call"

def has_tool_use(conv_list: List[Dict[str, Any]]) -> bool:
    return any(isinstance(m, dict) and m.get("from") in ("function_call", "observation") for m in conv_list)

def truncate_observation_inplace(data: Dict[str, Any], max_len: int) -> None:
    """å¯¹ observation çš„ value è¿›è¡Œé•¿åº¦æˆªæ–­ï¼ˆä»…å¯¼å‡ºæ—¶ç”Ÿæ•ˆï¼Œä¸æ”¹æ•°æ®åº“ï¼‰ã€‚"""
    if max_len <= 0:
        return
    convs = data.get("conversations", [])
    if not isinstance(convs, list):
        return
    for m in convs:
        if isinstance(m, dict) and m.get("from") == "observation":
            v = m.get("value")
            if isinstance(v, str) and len(v) > max_len:
                m["value"] = v[:max_len] + "\n... [truncated at export] ..."

def fetch_rows(conn: sqlite3.Connection, table: str, model_filter: str = "") -> List[Tuple[str, str]]:
    """ä»æŒ‡å®šè¡¨è·å– conversation åˆ—å†…å®¹ã€‚"""
    cur = conn.cursor()
    if table == "confirmed_interactions":
        base_sql = "SELECT model, conversation FROM confirmed_interactions ORDER BY confirmed_timestamp"
    else:
        base_sql = "SELECT model, conversation FROM interactions ORDER BY timestamp"
    if model_filter:
        cur.execute(base_sql)
        all_rows = cur.fetchall()
        # è§„èŒƒåŒ–ä¸ºå­—ç¬¦ä¸²å…ƒç»„ï¼Œé¿å… None è§¦å‘ç±»å‹å‘Šè­¦
        norm_rows: List[Tuple[str, str]] = [(str(m or ""), str(c or "")) for (m, c) in all_rows]
        return [rc for rc in norm_rows if model_filter in rc[0]]
    cur.execute(base_sql)
    # æ˜¾å¼æ”¶æ•›ç±»å‹
    return cast(List[Tuple[str, str]], [(str(m or ""), str(c or "")) for (m, c) in cur.fetchall()])

def process_conversations(
    db_path: str = "interactions.db",
    table: str = "interactions",
    output_file: str = "conversations.jsonl",
    invalid_file: str = "invalid_conversations.jsonl",
    only_function_call_only: bool = False,
    truncate_observation: int = 0,
    model_filter: str = "",
    max_records: int = 0,
    out_format: str = "sharegpt",
    tools_schema_mode: str = "auto"
) -> None:
    """å¯¼å‡º + æ ¡éªŒï¼šä»æ•°æ®åº“å¯¼å‡º ShareGPT æ‰©å±•æ ¼å¼ JSONLï¼Œå¹¶è¾“å‡ºç»Ÿè®¡ã€‚"""
    conn = None
    if table not in ("interactions", "confirmed_interactions"):
        raise ValueError("table å¿…é¡»ä¸º interactions æˆ– confirmed_interactions")

    try:
        conn = sqlite3.connect(db_path)
        rows = fetch_rows(conn, table, model_filter)

        if not rows:
            print("âŒ æ²¡æœ‰åŒ¹é…çš„æ•°æ®")
            return

        valid_count = 0
        invalid_count = 0
        fc_only_count = 0
        with_tool_count = 0
        with_gpt_count = 0
        errors: List[str] = []
        total = 0

        with open(output_file, "w", encoding="utf-8") as f_ok, open(invalid_file, "w", encoding="utf-8") as f_bad:
            for idx, (model, conv_str) in enumerate(rows, 1):
                if max_records and valid_count + invalid_count >= max_records:
                    break
                total += 1
                try:
                    data = json.loads(conv_str)

                    # ç»Ÿä¸€ tools å­—æ®µä¸ºå­—ç¬¦ä¸²
                    convert_tools_to_string(data)

                    # æ ¡éªŒç»“æ„
                    validate_sharegpt(data)

                    # æŒ‡æ ‡ç»Ÿè®¡
                    conv_list = data.get("conversations", [])
                    if has_tool_use(conv_list):
                        with_tool_count += 1
                    if any(isinstance(m, dict) and m.get("from") in ("gpt", "assistant") for m in conv_list):
                        with_gpt_count += 1

                    fc_only = is_function_call_only(conv_list)
                    if only_function_call_only and not fc_only:
                        # è¿‡æ»¤æ‰é function_call-only
                        continue
                    if fc_only:
                        fc_only_count += 1

                    # å¯¼å‡ºå‰å¯¹ observation æˆªæ–­ï¼ˆä¸å½±å“åŸæ•°æ®ï¼‰
                    if truncate_observation > 0:
                        truncate_observation_inplace(data, truncate_observation)

                    # å†™å…¥æœ‰æ•ˆæ ·æœ¬ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
                    if out_format == "openai":
                        msgs = sharegpt_to_openai_messages(conv_list, data.get("system", ""))
                        out_obj = {"messages": msgs}
                        # å½“ä¸”ä»…å½“å­˜åœ¨å‡½æ•°è°ƒç”¨æ—¶ï¼ŒæŒ‰ç­–ç•¥è¾“å‡º tools
                        has_tool_calls = any(isinstance(m, dict) and m.get("role") == "assistant" and m.get("tool_calls") for m in msgs)
                        if has_tool_calls:
                            tools_list = derive_tools_schema(conv_list, tools_schema_mode, data.get("tools"))
                            if tools_list:
                                out_obj["tools"] = tools_list

                        f_ok.write(json.dumps(out_obj, ensure_ascii=False) + "\n")
                    else:
                        f_ok.write(json.dumps(data, ensure_ascii=False) + "\n")
                    valid_count += 1
                except Exception as e:
                    invalid_count += 1
                    # åŸæ ·å†™å‡ºæ— æ•ˆè®°å½•ï¼Œä¾¿äºåç»­æ’æŸ¥
                    f_bad.write(conv_str + "\n")
                    if len(errors) < 20:
                        errors.append(f"è®°å½• {idx}: {repr(e)}")

        print("âœ… å¯¼å‡ºå®Œæˆ")
        print(f"ğŸ“¦ æ¥æºè¡¨: {table}")
        print(f"ğŸ§® æ€»è¯»å–æ¡æ•°: {total}")
        print(f"ğŸŸ¢ æœ‰æ•ˆ: {valid_count}  | ğŸ”´ æ— æ•ˆ: {invalid_count}")
        if valid_count:
            print(f"ğŸ› ï¸ å«å·¥å…·è°ƒç”¨: {with_tool_count} ({with_tool_count / max(valid_count,1):.1%} of valid)")
            print(f"ğŸ—£ï¸ å«åŠ©æ‰‹æ–‡æœ¬: {with_gpt_count} ({with_gpt_count / max(valid_count,1):.1%} of valid)")
            print(f"ğŸ§© function_call-only: {fc_only_count} ({fc_only_count / max(valid_count,1):.1%} of valid)")
        if errors:
            print("\néƒ¨åˆ†é”™è¯¯æ ·ä¾‹ï¼ˆæœ€å¤š20æ¡ï¼‰ï¼š")
            for e in errors:
                print("-", e)
        if os.path.exists(output_file):
            print(f"\nğŸ“ å¯¼å‡ºæ–‡ä»¶: {output_file}  å¤§å°: {os.path.getsize(output_file)/1024:.2f} KB")
        if os.path.exists(invalid_file):
            print(f"ğŸ§¾ æ— æ•ˆæ ·æœ¬: {invalid_file}  å¤§å°: {os.path.getsize(invalid_file)/1024:.2f} KB")

    finally:
        try:
            conn and conn.close()
        except Exception:
            pass

def parse_args():
    p = argparse.ArgumentParser(description="å¯¼å‡º/æ ¡éªŒ ShareGPT æ‰©å±•æ•°æ®ï¼ˆæ”¯æŒ function_call/observationï¼‰")
    p.add_argument("--db", default="interactions.db", help="æ•°æ®åº“è·¯å¾„")
    p.add_argument("--table", default="interactions", choices=["interactions", "confirmed_interactions"], help="å¯¼å‡ºæ¥æºè¡¨")
    p.add_argument("--output", default="conversations.jsonl", help="æœ‰æ•ˆæ ·æœ¬å¯¼å‡ºæ–‡ä»¶")
    p.add_argument("--invalid", default="invalid_conversations.jsonl", help="æ— æ•ˆæ ·æœ¬å¯¼å‡ºæ–‡ä»¶")
    p.add_argument("--only-function-call-only", action="store_true", help="ä»…å¯¼å‡º function_call-only æ ·æœ¬")
    p.add_argument("--truncate-observation", type=int, default=0, help="å¯¹ observation çš„ value è¿›è¡Œé•¿åº¦æˆªæ–­ï¼ˆ0 è¡¨ç¤ºä¸æˆªæ–­ï¼‰")
    p.add_argument("--model-filter", default="", help="æŒ‰æ¨¡å‹ååŒ…å«åŒ¹é…è¿‡æ»¤ï¼ˆç®€å•åŒ…å«åŒ¹é…ï¼‰")
    p.add_argument("--max-records", type=int, default=0, help="æœ€å¤šå¤„ç†å¤šå°‘æ¡ï¼ˆ0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰")
    p.add_argument("--format", default="sharegpt", choices=["sharegpt", "openai"], help="å¯¼å‡ºæ ¼å¼ï¼šsharegpt æˆ– openai")
    p.add_argument("--tools-schema", default="auto", choices=["auto", "yes", "no", "derive"], help="tools ç­¾åç­–ç•¥ï¼šauto(é»˜è®¤)/yes/no/derive")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    if not os.path.exists(args.db):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶: {args.db}")
        raise SystemExit(1)
    print("ğŸš€ å¼€å§‹å¯¼å‡º/æ ¡éªŒ ...")
    process_conversations(
        db_path=args.db,
        table=args.table,
        output_file=args.output,
        invalid_file=args.invalid,
        only_function_call_only=args.only_function_call_only,
        truncate_observation=args.truncate_observation,
        model_filter=args.model_filter,
        max_records=args.max_records,
        out_format=args.format,
        tools_schema_mode=args.tools_schema
    )
    print("ğŸ‰ å®Œæˆ")